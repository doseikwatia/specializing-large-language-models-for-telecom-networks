{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "ROOT_DIR = osp.dirname(os.getcwd())\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'microsoft/phi-2'\n",
    "CONTEXT_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_STORE_PATH = '../data/vectorstore/'\n",
    "EMBEDDING_MODEL_NAME='BAAI/bge-small-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever():\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True})\n",
    "\n",
    "    vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH+\"chromadb\", embedding_function=embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={'k': 15}\n",
    "    )\n",
    "   \n",
    "    rerank_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\", model_kwargs = {'device': 'cuda'})\n",
    "\n",
    "    compressor = CrossEncoderReranker(model=rerank_model, top_n=3)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )\n",
    "    return compression_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.prompt import get_training_prompt\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "\n",
    "CHUNK_SIZE=512\n",
    "GENERATE_PROMPTS=False\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename) as json_file :\n",
    "        json_data = json.load(json_file)\n",
    "    return json_data\n",
    "\n",
    "data = read_data(\"../data/TeleQnA_training.txt\")\n",
    "\n",
    "def get_prompt(qstn_datas):\n",
    "    retriever = create_retriever()\n",
    "    prompts = []\n",
    "    for qstn_data in tqdm(qstn_datas):\n",
    "        qstn_text = qstn_data['question']\n",
    "        docs = retriever.invoke(qstn_text)\n",
    "        context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "        prompt = get_training_prompt(qstn_data,context)\n",
    "        prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "def chunks(container,size):\n",
    "    for i in range(0, len(container), size):\n",
    "        yield container[i:i + size]\n",
    "        \n",
    "def flatten(container):\n",
    "    result = []\n",
    "    for chunk in container:\n",
    "        result += chunk\n",
    "    return result \n",
    "           \n",
    "# finetuning_datalist = list(map(lambda entry:get_prompt(entry[1],create_retriever()),tqdm(data.items())))\n",
    "if GENERATE_PROMPTS:\n",
    "    finetuning_datalist = flatten(Parallel(n_jobs=4)(delayed(get_prompt)(list(map(lambda e:e[1],entry))) for entry in tqdm(chunks(list(data.items()),CHUNK_SIZE))))\n",
    "    with open('../bin/pickle/finetuning_datalist.pkl','wb') as bin_file:\n",
    "        pickle.dump(finetuning_datalist,bin_file)\n",
    "else:\n",
    "    with open('../bin/pickle/finetuning_datalist.pkl','rb') as bin_file:\n",
    "        finetuning_datalist=pickle.load(bin_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Prompt\n",
      "********************************************************************************\n",
      "### Instructions: \n",
      "Based on the provided context, select the correct answer from the choices given. Provide your answer in the following format: option Number) Answer.\n",
      "\n",
      "Context:\n",
      "4.2.2.3.1\tGeneral. . The Nmfaf_3daDataManagement_Deconfigure service operation is used by an NF service consumer to stop mapping data or analytics received by the MFAF to one or more out-bound notification endpoints. -\t3GPP DCCF Adaptor (3DA) Data Management Service: Nmfaf_3daDataManagement Service enables the DCCF to convey to the Messaging Framework, information about the data the Messaging Framework will receive from a Data Source, formatting and processing 9.2.2\tNmfaf_3daDataManagement_Configure service operation. . Service operation name: Nmfaf_3daDataManagement_Configure a short description of their use within the Nmfaf_3daDataManagement service based interface. NWDAF, PCF, NSSF, AMF, SMF, NEF, AF. . NOTE:\tThe services correspond to the Nmfaf_3caDataManagement service and Nmfaf_3daDataManagement service as defined in 3GPPÂ TSÂ 23.288Â [14].\n",
      "\n",
      "Question:\n",
      "What is the purpose of the Nmfaf_3daDataManagement_Deconfigure service operation? [3GPP Release 18]\n",
      "\n",
      "Choices:\n",
      "option 1: To configure the MFAF to map data or analytics received by the MFAF to out-bound notification endpoints\n",
      "option 2: To configure the MFAF to stop mapping data or analytics received by the MFAF to out-bound notification endpoints\n",
      "option 3: To supply data or analytics from the MFAF to notification endpoints\n",
      "option 4: To fetch data or analytics from the MFAF based on fetch instructions\n",
      "\n",
      "\n",
      "### Answer:\n",
      "option 2: To configure the MFAF to stop mapping data or analytics received by the MFAF to out-bound notification endpoints\n",
      "\n",
      "### Explanation:\n",
      "The Nmfaf_3daDataManagement_Deconfigure service operation is used to stop mapping data or analytics received by the MFAF to one or more out-bound notification endpoints.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"********************************************************************************\n",
    "Prompt\n",
    "********************************************************************************\n",
    "{finetuning_datalist[0]['prompt']}\"\"\")\n",
    "\n",
    "# ********************************************************************************\n",
    "# Correct answer\n",
    "# ********************************************************************************\n",
    "# {finetuning_datalist[0]['answer']}\n",
    "\n",
    "# ********************************************************************************\n",
    "# Explanation\n",
    "# ********************************************************************************\n",
    "# {finetuning_datalist[0]['explanation']}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finetuning_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the largest token count\n",
    "def get_max_length(finetuning_datalist,tokenizer):\n",
    "    tokens = tokenizer(list(map(lambda e: e['prompt'],finetuning_datalist)),return_tensors='np') #+e['answer']+'\\n'+e['explanation']\n",
    "    argmax_token_len = np.argmax([t.shape[0] for t in tokens.data['input_ids']])\n",
    "    max_length = tokens.data['input_ids'][argmax_token_len].shape[0]\n",
    "    max_length = min(max_length, CONTEXT_LENGTH)\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data for training\n",
    "def tokenize_dataset(example, tokenizer, max_length):\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    text = example['prompt'][0] #+ example['answer'][0] + '\\n'+example['explanation'][0]\n",
    "    # print(text)\n",
    "    tokenized_input = tokenizer(\n",
    "        text,\n",
    "        max_length = max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    return tokenized_input\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['prompt'])):\n",
    "        text = example['prompt'][i] #+ example['answer'][i] + '\\n'+example['explanation'][i]\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length= get_max_length(finetuning_datalist, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset = Dataset.from_list(finetuning_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer', 'explanation'],\n",
       "    num_rows: 1461\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1461/1461 [00:03<00:00, 458.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = finetuning_dataset.map(\n",
    "    lambda e: tokenize_dataset(e,tokenizer, max_length),\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer', 'explanation', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1461\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'answer', 'explanation', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1314\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'answer', 'explanation', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 147\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1314/1314 [00:00<00:00, 9863.76 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:00<00:00, 2846.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "split_dataset.save_to_disk(\"../data/finetuning/split_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "from lib.prompt import train_response_template, train_instruction_template, train_explanation_template\n",
    "from trl import  SFTTrainer, DataCollatorForCompletionOnlyLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": MODEL_NAME,\n",
    "        \"max_length\" : CONTEXT_LENGTH\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": False,\n",
    "        \"path\": \"../data/finetuning/split_dataset/\"\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loftq_config = LoftQConfig(loftq_bits=4)           # set 4bit quantization\n",
    "lora_config = LoraConfig(\n",
    "    init_lora_weights=\"loftq\",\n",
    "    loftq_config=loftq_config,\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=8,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir='../bin/',\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=64, # Number of update steps between two evaluations\n",
    "  save_steps=64, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/trl/trainer/utils.py:116: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=train_instruction_template, response_template=train_response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(split_dataset['train'][0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    max_seq_length=max_length,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    data_collator=collator,\n",
    "    formatting_func=formatting_prompts_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='512' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [512/512 1:01:42, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.542540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.546100</td>\n",
       "      <td>0.492168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.454791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.617100</td>\n",
       "      <td>0.433162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.418728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.410427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.406077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.720400</td>\n",
       "      <td>0.404911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=512, training_loss=0.49589239968918264, metrics={'train_runtime': 3708.0232, 'train_samples_per_second': 0.552, 'train_steps_per_second': 0.138, 'total_flos': 1.430739256624128e+16, 'train_loss': 0.49589239968918264, 'epoch': 1.5585996955859969})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model.save_pretrained('../bin/pretrained_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
