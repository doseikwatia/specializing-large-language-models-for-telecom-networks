{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "ROOT_DIR = osp.dirname(os.getcwd())\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'microsoft/phi-2'\n",
    "CONTEXT_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.prompt import get_training_prompt\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename) as json_file :\n",
    "        json_data = json.load(json_file)\n",
    "    return json_data\n",
    "\n",
    "data = read_data(\"../data/TeleQnA_training.txt\")\n",
    "finetuning_datalist = list(map(lambda entry:get_training_prompt(entry[1]),data.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Prompt\n",
      "********************************************************************************\n",
      "### Instructions: \n",
      "Based on only the provided context, select the correct answer from the choices given. Provide your answer in the following format: option Number) Answer. Do not include any additional text or explanation.\n",
      "\n",
      "Context:\n",
      "The Nmfaf_3daDataManagement_Deconfigure service operation is used to stop mapping data or analytics received by the MFAF to one or more out-bound notification endpoints.\n",
      "\n",
      "Question:\n",
      "What is the purpose of the Nmfaf_3daDataManagement_Deconfigure service operation? [3GPP Release 18]\n",
      "\n",
      "Choices:\n",
      "option 1) To configure the MFAF to map data or analytics received by the MFAF to out-bound notification endpoints\n",
      "option 2) To configure the MFAF to stop mapping data or analytics received by the MFAF to out-bound notification endpoints\n",
      "option 3) To supply data or analytics from the MFAF to notification endpoints\n",
      "option 4) To fetch data or analytics from the MFAF based on fetch instructions\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "Correct answer\n",
      "********************************************************************************\n",
      "option 2: To configure the MFAF to stop mapping data or analytics received by the MFAF to out-bound notification endpoints\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"********************************************************************************\n",
    "Prompt\n",
    "********************************************************************************\n",
    "{finetuning_datalist[0]['question']}\n",
    "\n",
    "\n",
    "********************************************************************************\n",
    "Correct answer\n",
    "********************************************************************************\n",
    "{finetuning_datalist[0]['answer']}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the largest token count\n",
    "def get_max_length(finetuning_datalist,tokenizer):\n",
    "    tokens = tokenizer(list(map(lambda e: e['question']+e['answer'],finetuning_datalist)),return_tensors='np')\n",
    "    argmax_token_len = np.argmax([t.shape[0] for t in tokens.data['input_ids']])\n",
    "    max_length = tokens.data['input_ids'][argmax_token_len].shape[0]\n",
    "    max_length = min(max_length, CONTEXT_LENGTH)\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data for training\n",
    "def tokenize_dataset(example, tokenizer, max_length):\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    text = example['question'][0]+example['answer'][0]\n",
    "    # print(text)\n",
    "    tokenized_input = tokenizer(\n",
    "        text,\n",
    "        max_length = max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    return tokenized_input\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        text = example['question'][i] + example['answer'][i]\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length= get_max_length(finetuning_datalist, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset = Dataset.from_list(finetuning_datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 1461\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1461/1461 [00:02<00:00, 596.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = finetuning_dataset.map(\n",
    "    lambda e: tokenize_dataset(e,tokenizer, max_length),\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1461\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1314\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 147\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1314/1314 [00:00<00:00, 29970.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 147/147 [00:00<00:00, 12066.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "split_dataset.save_to_disk(\"../data/finetuning/split_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "from lib.prompt import train_response_template, train_instruction_template\n",
    "from trl import  SFTTrainer, DataCollatorForCompletionOnlyLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": MODEL_NAME,\n",
    "        \"max_length\" : CONTEXT_LENGTH\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": False,\n",
    "        \"path\": \"../data/finetuning/split_dataset/\"\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loftq_config = LoftQConfig(loftq_bits=4)           # set 4bit quantization\n",
    "lora_config = LoraConfig(\n",
    "    init_lora_weights=\"loftq\",\n",
    "    loftq_config=loftq_config,\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir='../bin/',\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=20, # Number of update steps between two evaluations\n",
    "  save_steps=20, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/trl/trainer/utils.py:116: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=train_instruction_template, response_template=train_response_template, tokenizer=tokenizer)\n",
    "\n",
    "# split_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    max_seq_length=1024,\n",
    "    train_dataset=split_dataset['train'],\n",
    "    eval_dataset=split_dataset['test'],\n",
    "    data_collator=collator,\n",
    "    formatting_func=formatting_prompts_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 18:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.699900</td>\n",
       "      <td>0.902585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.760500</td>\n",
       "      <td>0.822143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.743702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.666733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>0.597731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.540387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>0.493923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.462277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.471600</td>\n",
       "      <td>0.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.435020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.6308332908898592, metrics={'train_runtime': 1121.6743, 'train_samples_per_second': 0.713, 'train_steps_per_second': 0.178, 'total_flos': 2371330057728000.0, 'train_loss': 0.6308332908898592, 'epoch': 0.60882800608828})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model.save_pretrained('../bin/pretrained/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
