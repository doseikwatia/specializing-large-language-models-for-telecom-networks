{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "ROOT_DIR = osp.dirname(os.getcwd())\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models, vectorstore and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from tqdm import tqdm\n",
    "from lib.config import LLM_MODEL_NAME, EMBEDDING_MODEL_NAME, VECTOR_STORE_NAME,COMPRESSION_RETRIEVER_TOP_N,VECTOR_RETRIEVER_K,RERANKER_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_PATH='../data/rel18/'\n",
    "VECTOR_STORE_PATH = '../data/vectorstore/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    device='gpu',\n",
    ")\n",
    "\n",
    "# embeddings=HuggingFaceBgeEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH+VECTOR_STORE_NAME, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.llamafile import Llamafile\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever, LineListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(\"langchain.retrievers.multi_query\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "# class LineList(BaseModel):\n",
    "#     # \"lines\" is the key (attribute name) of the parsed output\n",
    "#     lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "# class LineListOutputParser(PydanticOutputParser):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__(pydantic_object=LineList)\n",
    "\n",
    "#     def parse(self, text: str) -> LineList:\n",
    "#         logger.info(text)\n",
    "#         lines = list(filter(lambda t: t.strip() != '',text.strip().split(\"\\n\")))\n",
    "#         return LineList(lines=lines)\n",
    "\n",
    "\n",
    "# output_parser = LineListOutputParser()\n",
    "\n",
    "# QUERY_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"\"\"<|system|>\n",
    "#     You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines.</s>\n",
    "#     <|user|>\n",
    "#     {question}</s>\n",
    "#     <|assistant|>\"\"\")\n",
    "\n",
    "# llm =  Llamafile()\n",
    "# # Chain\n",
    "# llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR_RETRIEVER_K = 50\n",
    "# COMPRESSION_RETRIEVER_TOP_N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstore_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {'k': VECTOR_RETRIEVER_K, } \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# tiny_token = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "# ]\n",
    "# prompt = tiny_token.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#self \n",
    "# metadata_field_info = [\n",
    "#     AttributeInfo(\n",
    "#         name=\"source\",\n",
    "#         description=\"The name of the document where the content was taken from\",\n",
    "#         type=\"string\",\n",
    "#     ),\n",
    "#     AttributeInfo(\n",
    "#         name=\"year\",\n",
    "#         description=\"The year the standard was released\",\n",
    "#         type=\"integer\",\n",
    "#     ),\n",
    "#     AttributeInfo( #3GPP Release\n",
    "#         name=\"tag\",\n",
    "#         description=\"The 3GPP release information\",\n",
    "#         type=\"string\",\n",
    "#     ),\n",
    "# ]\n",
    "# self_llm = CustomTransformersLLM(model_name='microsoft/phi-2', max_length=128) \n",
    "# document_content_description = 'contains technical details about telecommunications standards'\n",
    "# self_retriever = SelfQueryRetriever.from_llm(\n",
    "#     self_llm,\n",
    "#     vectorstore,\n",
    "#     document_content_description,\n",
    "#     metadata_field_info,\n",
    "#     structured_query_translator= ChromaTranslator()\n",
    "# )\n",
    "\n",
    "\n",
    "#compression\n",
    "\n",
    "\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL_NAME, model_kwargs = {'device': 'cuda'})\n",
    "\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=COMPRESSION_RETRIEVER_TOP_N)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vstore_retriever\n",
    ")\n",
    "\n",
    "\n",
    "# multiquery\n",
    "# llm = Llamafile()\n",
    "# retriever_from_llm = MultiQueryRetriever.from_llm(llm=llm,\n",
    "#     retriever=vstore_retriever\n",
    "# )\n",
    "# retriever_from_llm = MultiQueryRetriever(\n",
    "#     retriever=compression_retriever, llm_chain=llm_chain, include_original=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = compression_retriever.invoke(\"What does the NEF notify to the AF after determining the suitable DNAI(s)? [3GPP Release 18]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='4a.\\tIf late notification via NEF is requested by the AF, the SMF notifies the NEF of the target DNAI of the PDU Session or indication of EAS rediscovery and may indicate capability of supporting EAS IP replacement in 5GC by invoking Nsmf_EventExposure_Notify service operation. The SMF may provide the target AF ID if it determines that the target DNAI is not supported by the source AF.', metadata={'source': '23502-i40.docx'}),\n",
       " Document(page_content='When NEF gets the AF request for determine DNAI information with the AF/EAS IP address or AF/EAS IP address range, optionally with the target geographic area, DNN, S-NSSAI. The NEF can map the geographic area into TA list. The NEF may find the target SMF(s) through NRF, with using the TA list, DNN, S-NSSAI. Then, the NEF requests the SMF with the AF/EAS IP address range, optionally TA list, DNN, S-NSSAI, in order to find the suitable DNAI(s).', metadata={'source': '23700-48-i00.docx'}),\n",
       " Document(page_content='1.\\tAF invokes Nnef_EventExposure service to subscribe for DNAI determination. The request includes EAS IP address or IP address range, may include geographic area. AF may also subscribe the DNAI information modification in NEF.\\n\\n2.\\tNEF finds the corresponding SMF based on the S-NSSAI, DNN, TA. Then, NEF invokes the Nsmf_EventExposure service to get the suitable DNAI(s) based on the EAS IP address or IP address range, TA list, S-NSSAI or DNN. NEF can also subscribe the DNAI information modification in SMF.', metadata={'source': '23700-48-i00.docx'}),\n",
       " Document(page_content='4d.\\tWhen the AF receives either the Nnef_TrafficInfluence_Notify message or the Nsmf_EventExposure_Notify message, the AF checks whether it can serve the target DNAI. If the AF instance change is needed, the AF determines the proper target AF for the target DNAI (e.g. based on locally configured information or the AF ID provided by the SMF in step\\xa04a or 4c) and performs the AF migration.', metadata={'source': '23502-i40.docx'}),\n",
       " Document(page_content='NOTE\\xa02:\\tFor the reporting of candidate DNAIs from SMF/NEF to AF, only early notification is used.\\n\\n2b.\\tWhen the NEF receives Nsmf_EventExposure_Notify, the NEF performs information mapping (e.g. AF Transaction Internal ID provided in Notification Correlation ID to AF Transaction ID, SUPI to GPSI, etc.) as applicable according to clause\\xa05.6.7 of TS\\xa023.501\\xa0[2] and triggers the appropriate Nnef_TrafficInfluence_Notify message. In this case, step\\xa02c is not applicable.', metadata={'source': '23502-i40.docx'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(VECTOR_RETRIEVER_K)\n",
    "print(COMPRESSION_RETRIEVER_TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "#Uncomment\n",
    "answer_model_name = LLM_MODEL_NAME\n",
    "tokenizer = AutoTokenizer.from_pretrained(answer_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "# base_model = AutoModelForCausalLM.from_pretrained(answer_model_name,device_map=\"auto\",)\n",
    "# answer_model = PeftModel.from_pretrained(base_model, '../bin/pretrained_256_64/', device_map=\"auto\")\n",
    "# pretrained_512_32\n",
    "answer_model = AutoModelForCausalLM.from_pretrained('../bin/pretrained_512_32/',device_map=\"auto\",)\n",
    "answer_generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=answer_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.prompt import get_inference_prompt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(qst_filename, answer_model, retriever, max_new_tokens=4,return_full_text=False, batch_size = 128):\n",
    "    with open(qst_filename) as file:\n",
    "        questions = json.load(file)\n",
    "    solutions = []\n",
    "    def get_question_prompt(qstn_id,qstn_data): # in tqdm(questions.items()):\n",
    "        qstn_id=qstn_id.split(' ')[1].strip()\n",
    "        qstn_text = qstn_data['question']\n",
    "        #searching through datastore for context\n",
    "        docs = retriever.invoke(qstn_text)\n",
    "        context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "        infer_data = get_inference_prompt(qstn_data, context)\n",
    "        prompt = infer_data['prompt']\n",
    "        return qstn_id,prompt\n",
    "    prompts = list(map(lambda entry:get_question_prompt(entry[0],entry[1]),tqdm(list(questions.items()))))\n",
    "    print(prompts[0])\n",
    "    num_prompts = len(prompts)\n",
    "    for i in tqdm(range(0,num_prompts,batch_size)):\n",
    "        current_prompts=list(map(lambda e:e[1],prompts[i:i+batch_size]))\n",
    "        current_qstn_ids=list(map(lambda e:e[0],prompts[i:i+batch_size]))\n",
    "        responses = answer_model(current_prompts,max_new_tokens=max_new_tokens, return_full_text=return_full_text)\n",
    "        current_ans_ids =list(map(lambda r:r[0]['generated_text'].split(':')[0][-1:].strip(),responses))\n",
    "        solutions += list(zip(current_qstn_ids,current_ans_ids))\n",
    "        \n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solution(filename,solution, task=''):\n",
    "    df = pd.DataFrame(solution,columns=['Question_ID','Answer_ID'])\n",
    "    df['Task'] = task\n",
    "    df.to_csv(filename,index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [02:48<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('8138', '### Instructions: \\nBased on the provided context, select the correct answer from the choices given. Provide your answer in the following format: option Number: Answer.\\n\\nContext:\\nIf a gNB initiates a channel occupancy using the channel access procedures described in clause 4.4.1 on a channel, the gNB may transmit a DL transmission(s) on the channel within the maximum Channel Occupancy Time described in Clause 4.4.1 on the channel after the DL transmission(s) initiating the channel occupancy. The followings are applicable to the DL transmission(s): -\\tIf the gap is  or , the gNB can transmit the transmission on the channel after performing Type 2A or Type 2B DL channel access procedures as described in clause 4.1.2.1 and 4.1.2.2, respectively.. . For the case where a gNB shares a channel occupancy initiated by a UE with configured grant PUSCH transmission, the gNB may transmit a transmission that follows the configured grant PUSCH transmission by the UE as follows: A channel occupancy that is initiated by a gNB and is shared with UE(s), satisfies the following:. . -\\tThe gNB shall transmit a DL transmission burst starting at the beginning of a period of duration  in which the channel occupancy is initiated immediately after sensing the channel to be idle for at least a sensing slot duration . If the channel is sensed to be busy, the gNB shall not perform any transmission during the current period. -\\tThe gNB may transmit a DL transmission burst(s) within the channel occupancy time immediately after sensing the channel to be idle for at least a sensing slot duration  if the gap between the DL transmission burst(s) and any previous transmission burst is more than .. . -\\tThe gNB may transmit DL transmission burst(s) after UL transmission burst(s) within the channel occupancy time without sensing the channel if the gap between the DL and UL transmission bursts is at most 4.4.4\\tChannel access procedures in an initiated channel occupancy. . If a gNB/UE initiates a channel occupancy using the channel access procedures described in clause 4.4.1 on a channel, the gNB/UE may transmit a DL/UL transmission(s) that is followed by a UL/DL transmission(s) within the maximum Channel Occupancy Time described in Clause 4.4.1. The followings are applicable to the UL/DL transmission(s):\\n\\nQuestion:\\nWhen can a gNB transmit a DL transmission(s) on a channel after initiating a channel occupancy? [3GPP Release 17]\\n\\nChoices:\\noption 1: Regardless of the duration of the gap between the DL transmission(s) and any previous transmission(s) corresponding to the channel occupancy initiated by the gNB.\\noption 2: If the gap between the DL transmission(s) and any previous transmission(s) corresponding to the channel occupancy initiated by the gNB is more than a threshold.\\noption 3: Both option 1 and option 2\\noption 4: None of the above\\n\\n\\n### Answer:')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [08:18<00:00, 166.19s/it]\n"
     ]
    }
   ],
   "source": [
    "train_soln = answer_questions('../data/TeleQnA_testing1.txt',answer_generator,compression_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_solution('testing_result.csv',train_soln,'Phi-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv('training_result.csv')\n",
    "act = pd.read_csv('../data/Q_A_ID_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['Answer_ID']=pred['Answer_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7761806981519507"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred['Answer_ID'] == act['Answer_ID']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|algo|score|\n",
    "|--|--|\n",
    "|similarity|0.6235455167693361|\n",
    "|mmr|xxx|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chromadb_512_32'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_STORE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_ID</th>\n",
       "      <th>Answer_ID</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>Phi-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>Phi-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>Phi-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>Phi-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>Phi-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Question_ID  Answer_ID   Task\n",
       "6            23          3  Phi-2\n",
       "14           53          1  Phi-2\n",
       "15           60          1  Phi-2\n",
       "16           70          1  Phi-2\n",
       "20           80          1  Phi-2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[pred['Answer_ID'] != act['Answer_ID']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context from datastore\n",
    "# question = q\n",
    "# docs = compression_retriever.invoke(question['question'])\n",
    "# context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "# prompt = get_inference_prompt(question, context)\n",
    "# refined_prompt = prompt['question']\n",
    "# answer = prompt['answer']\n",
    "# print(refined_prompt)\n",
    "# gen_result = answer_generator(refined_prompt,max_new_tokens=128,return_full_text=False,)\n",
    "# \"option 1) Avoid monitoring neighbo\"\n",
    "# print(answer)\n",
    "# print(gen_result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
