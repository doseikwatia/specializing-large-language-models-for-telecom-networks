{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "ROOT_DIR = osp.dirname(os.getcwd())\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models, vectorstore and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_PATH='../data/rel18/'\n",
    "VECTOR_STORE_PATH = '../data/vectorstore/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "encode_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    device='cpu',\n",
    "    gpt4all_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH+\"chromadb\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['00000a5c-dc75-47d5-83c5-4763f5cf0ef1'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'source': '../data/rel18/28550-i30.docx'}],\n",
       " 'documents': ['The Performance Data Stream Units are described using ASN.1 as specified in ITU-T Rec. X.680 [15] and X.681 [16]. Transfer syntax for Performance Data Stream Units is derived from their ASN.1 definitions by use of Packed Encoding Rules (PER), aligned as'],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.get(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_model = 'microsoft/phi-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(answer_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "answer_model = AutoModelForCausalLM.from_pretrained(answer_model,device_map=\"auto\",)\n",
    "peft_model = PeftModel.from_pretrained(answer_model, '../bin/pretrained/', device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "answer_generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rerank_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 50}\n",
    ")\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=10)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.prompt import get_inference_prompt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(qst_filename, answer_model, retriever, max_new_tokens=4,return_full_text=False, batch_size = 128):\n",
    "    with open(qst_filename) as file:\n",
    "        questions = json.load(file)\n",
    "    solutions = []\n",
    "    def get_question_prompt(qstn_id,qstn_data): # in tqdm(questions.items()):\n",
    "        qstn_id=qstn_id.split(' ')[1].strip()\n",
    "        qstn_text = qstn_data['question']\n",
    "        #searching through datastore for context\n",
    "        docs = retriever.invoke(qstn_text)\n",
    "        context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "        infer_data = get_inference_prompt(qstn_data, context)\n",
    "        prompt = infer_data['question']\n",
    "        return qstn_id,prompt\n",
    "    prompts = list(map(lambda entry:get_question_prompt(entry[0],entry[1]),tqdm(list(questions.items()))))\n",
    "    num_prompts = len(prompts)\n",
    "    for i in tqdm(range(0,num_prompts,batch_size)):\n",
    "        current_prompts=list(map(lambda e:e[1],prompts[i:i+batch_size]))\n",
    "        current_qstn_ids=list(map(lambda e:e[0],prompts[i:i+batch_size]))\n",
    "        responses = answer_model(current_prompts,max_new_tokens=max_new_tokens, return_full_text=return_full_text)\n",
    "        current_ans_ids =list(map(lambda r:r[0]['generated_text'].split(')')[0][-1:].strip(),responses))\n",
    "        solutions += list(zip(current_qstn_ids,current_ans_ids))\n",
    "        \n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solution(filename,solution, task=''):\n",
    "    df = pd.DataFrame(solution,columns=['Question_ID','Answer_ID'])\n",
    "    df['Task'] = task\n",
    "    df.to_csv(filename,index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [02:08<00:00,  2.84it/s]\n",
      "100%|██████████| 3/3 [05:45<00:00, 115.12s/it]\n"
     ]
    }
   ],
   "source": [
    "train_soln = answer_questions('../data/TeleQnA_testing1.txt',answer_generator,compression_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_solution('testing_result.csv',train_soln,'Phi-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('testing_result.csv')['Answer_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv('./result.csv')\n",
    "act = pd.read_csv('../data/Q_A_ID_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['Answer_ID']=pred['Answer_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6344969199178645"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred['Answer_ID'] == act['Answer_ID']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context from datastore\n",
    "# docs = compression_retriever.invoke(question['question'])\n",
    "# context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "# question\n",
    "# prompt = get_inference_prompt(question, context)\n",
    "# refined_prompt = prompt['question']\n",
    "# answer = prompt['answer']\n",
    "# print(refined_prompt)\n",
    "# gen_result = answer_generator(refined_prompt,max_new_tokens=128,return_full_text=False,)\n",
    "# \"option 1) Avoid monitoring neighbo\"\n",
    "# print(answer)\n",
    "# print(gen_result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
