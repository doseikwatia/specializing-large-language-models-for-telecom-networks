{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "ROOT_DIR = osp.dirname(os.getcwd())\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models, vectorstore and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from tqdm import tqdm\n",
    "from lib.normic_wrapper import NomicEmbedding\n",
    "from lib.config import LLM_MODEL_NAME, EMBEDDING_MODEL_NAME, VECTOR_STORE_NAME,COMPRESSION_RETRIEVER_TOP_N,VECTOR_RETRIEVER_K,RERANKER_MODEL_NAME\n",
    "from lib.custom_retriever import CustomRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_PATH='../data/rel18/'\n",
    "VECTOR_STORE_PATH = '../data/vectorstore/'\n",
    "EMBEDDING_KWARGS = {'allow_download': 'True'}#,'dimensionality':512, 'prefix':'search_query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    gpt4all_kwargs =EMBEDDING_KWARGS,\n",
    "    device='gpu',\n",
    ")\n",
    "\n",
    "# embeddings=HuggingFaceBgeEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"}, encode_kwargs={\"normalize_embeddings\": True})\n",
    "# embeddings = NomicEmbedding(model_name=EMBEDDING_MODEL_NAME,dimensionality=512,device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH+VECTOR_STORE_NAME, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chromadb_512_32'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_STORE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorstore.get(limit=1,include=['embeddings'])['embeddings'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain_community.llms.llamafile import Llamafile\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever, LineListOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(\"langchain.retrievers.multi_query\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_community.llms.llamafile import Llamafile\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import  BaseOutputParser\n",
    "# from langchain.retrievers.multi_query import LineListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "# class LineList(BaseModel):\n",
    "#     # \"lines\" is the key (attribute name) of the parsed output\n",
    "#     lines: List[str] = Field(description=\"Lines of text\")\n",
    "    \n",
    "#     def append(self, item:str):\n",
    "#         self.lines.append(item)\n",
    "\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = list(filter(lambda s:s != '',list(map(lambda s:s.strip(),text.split(\"\\n\")))))\n",
    "        return lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate two different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines.\n",
    "    {question}\n",
    "    \"\"\")\n",
    "\n",
    "llm =  Llamafile(base_url='http://127.0.0.1:8080',seed=123)\n",
    "# # Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR_RETRIEVER_K = 50\n",
    "# COMPRESSION_RETRIEVER_TOP_N=2\n",
    "# RERANKER_MODEL_NAME='BAAI/bge-reranker-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.deduplicate_retriever import DeduplicateRetriever\n",
    "\n",
    "\n",
    "vstore_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs= {'k': VECTOR_RETRIEVER_K, }\n",
    ")\n",
    "vstore_retriever = DeduplicateRetriever(base_retriever=vstore_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# tiny_token = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "# ]\n",
    "# prompt = tiny_token.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "#self \n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The name of the document where the content was taken from\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the standard was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo( #3GPP Release\n",
    "        name=\"tag\",\n",
    "        description=\"The 3GPP release information\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "# self_llm = CustomTransformersLLM(model_name='microsoft/phi-2', max_length=128) \n",
    "document_content_description = 'contains technical details about telecommunications standards'\n",
    "self_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    structured_query_translator= ChromaTranslator()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# custom_retriever = CustomRetriever(compression_retriever,answer_generator,32)\n",
    "# multiquery\n",
    "# llm = Llamafile()\n",
    "# retriever_from_llm = MultiQueryRetriever.from_llm(llm=llm,\n",
    "#     retriever=vstore_retriever\n",
    "# )\n",
    "retriever_from_llm = MultiQueryRetriever(\n",
    "    retriever=compression_retriever, llm_chain=llm_chain, include_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/yolo/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#compression\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL_NAME, model_kwargs = {'device': 'cuda'})\n",
    "\n",
    "compressor = CrossEncoderReranker(model=rerank_model, top_n=COMPRESSION_RETRIEVER_TOP_N)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vstore_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does a supporting UE attach to the same core network operator from which it detached in a shared network? [3GPP Release 17]\n"
     ]
    }
   ],
   "source": [
    "question = \"How does a supporting UE attach to the same core network operator from which it detached in a shared network? [3GPP Release 17]\"\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = compression_retriever.invoke(question)\n",
    "# for d in docs:\n",
    "#     print(d.page_content)\n",
    "#     print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='If the UE can proceed to attach, it initiates the Attach procedure by the transmission, to the eNodeB, of an Attach Request (IMSI or old GUTI, Old GUTI type, last visited TAI (if available), UE Core Network Capability, UE Specific DRX parameters, extended idle mode DRX parameters, UE paging probability information, Attach Type, ESM message container (Request Type, PDN Type, Protocol Configuration Options, Ciphered Options Transfer Flag, Header Compression Configuration), KSIASME, NAS sequence number,' metadata={'source': '23401-i40.docx'}\n",
      "----\n",
      "page_content='with another UE is performed via the network.' metadata={'source': '23304-i40.docx'}\n",
      "----\n",
      "page_content='means that the communication with another UE is performed via the network.' metadata={'source': '23700-33-i00.docx'}\n",
      "----\n",
      "page_content='NOTE 1:\\tIn shared networks, when the message is sent from the VPLMN to the HPLMN, the PLMN ID that is communicated in this IE shall be that of the selected Core Network Operator for supporting UEs, or that of the allocated Core Network Operator for non-supporting UEs. As an exception, based on inter-operator roaming/sharing agreement, if the information on whether the UE is a supporting or non-supporting UE is available, the PLMN ID that is communicated to the HPLMN for non-supporting UEs shall be the' metadata={'source': '29274-i50.docx'}\n",
      "----\n",
      "page_content='-\\tA UE that is detaching from a specific access system and wants to preserve all or a subset of the active PDN connections that use that access system shall initiate UE initiated PDN disconnection procedure for each of the PDN connection which is not required to be preserved. The UE then shall initiate the applicable handover procedure to transfer to the access system through which the UE remains attached to the Evolved Packet Core each of the PDN connections to be preserved.' metadata={'source': '23402-i30.docx'}\n",
      "----\n",
      "page_content='NOTE 2:\\tIn shared networks, when the message is sent from the VPLMN to the HPLMN, the PLMN ID that is communicated in this IE shall be that of the selected Core Network Operator for supporting UEs, or that of the allocated Core Network Operator for non-supporting UEs. As an exception, based on inter-operator roaming/sharing agreement, if the information on whether the UE is a supporting or non-supporting UE is available, the PLMN ID that is communicated to the HPLMN for non-supporting UEs shall be the' metadata={'source': '29274-i50.docx'}\n",
      "----\n",
      "page_content='NOTE 3:\\tIn shared networks, when the message is sent from the VPLMN to the HPLMN, the PLMN ID that is communicated in this IE shall be that of the selected Core Network Operator for supporting UEs, or that of the allocated Core Network Operator for non-supporting UEs. As an exception, based on inter-operator roaming/sharing agreement, if the information on whether the UE is a supporting or non-supporting UE is available, the PLMN ID that is communicated to the HPLMN for non-supporting UEs shall be the' metadata={'source': '29274-i50.docx'}\n",
      "----\n",
      "page_content='NOTE 7:\\tIn shared networks, when the message is sent from the VPLMN to the HPLMN, the PLMN ID that is communicated in this IE shall be that of the selected Core Network Operator for supporting UEs, or that of the allocated Core Network Operator for non-supporting UEs. As an exception, based on inter-operator roaming/sharing agreement, if the information on whether the UE is a supporting or non-supporting UE is available, the PLMN ID that is communicated to the HPLMN for non-supporting UEs shall be the' metadata={'source': '29274-i50.docx'}\n",
      "----\n",
      "page_content='NOTE 10:\\tIn shared networks, when the message is sent from the VPLMN to the HPLMN, the PLMN ID that is communicated in this IE shall be that of the selected Core Network Operator for supporting UEs, or that of the allocated Core Network Operator for non-supporting UEs. As an exception, based on inter-operator roaming/sharing agreement, if the information on whether the UE is a supporting or non-supporting UE is available, the PLMN ID that is communicated to the HPLMN for non-supporting UEs shall be the' metadata={'source': '29274-i50.docx'}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "docs = compression_retriever.invoke(question)\n",
    "for d in docs:\n",
    "    print(d)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VECTOR_STORE_NAME)\n",
    "print(VECTOR_RETRIEVER_K)\n",
    "print(COMPRESSION_RETRIEVER_TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.prompt import get_mcq_inference_prompt\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "len({'j':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment\n",
    "answer_model_name = LLM_MODEL_NAME\n",
    "tokenizer = AutoTokenizer.from_pretrained(answer_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "# base_model = AutoModelForCausalLM.from_pretrained(answer_model_name,device_map=\"auto\",)\n",
    "# answer_model = PeftModel.from_pretrained(base_model, '../bin/pretrained_256_64/', device_map=\"auto\")\n",
    "# pretrained_512_32\n",
    "answer_model = AutoModelForCausalLM.from_pretrained('../bin/pretrained_512_32/',device_map=\"auto\",)\n",
    "# from huggingface\n",
    "# answer_model = AutoModelForCausalLM.from_pretrained(answer_model_name,device_map=\"auto\")\n",
    "\n",
    "answer_generator = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=answer_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# question_answering = transformers.pipeline(\n",
    "#     # \"question-answering\",\n",
    "#     model=\"deepset/roberta-base-squad2\",\n",
    "#     # tokenizer='google-bert/bert-base-cased',\n",
    "#     # torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"cuda:0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(qst_filename, answer_model, retriever, max_new_tokens=4,return_full_text=False, batch_size = 128, sample_size=-1):\n",
    "    with open(qst_filename) as file:\n",
    "        questions = json.load(file)\n",
    "    solutions = []\n",
    "    def get_question_prompt(qstn_id,qstn_data): # in tqdm(questions.items()):\n",
    "        qstn_id=qstn_id.split(' ')[1].strip()\n",
    "        qstn_text = qstn_data['question']\n",
    "        #searching through datastore for context\n",
    "        docs = retriever.invoke(qstn_text)\n",
    "        context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "        infer_data = get_mcq_inference_prompt(qstn_data, context)\n",
    "        prompt = infer_data['prompt']\n",
    "        return qstn_id,prompt\n",
    "    \n",
    "    if sample_size < 0 :\n",
    "        sampled_questions = list(questions.items())\n",
    "    else:\n",
    "        sampled_questions = random.sample(list(questions.items()),sample_size)\n",
    "        \n",
    "    prompts = list(map(lambda entry:get_question_prompt(entry[0],entry[1]),tqdm(sampled_questions)))\n",
    "    print(prompts[0])\n",
    "    num_prompts = len(prompts)\n",
    "    for i in tqdm(range(0,num_prompts,batch_size)):\n",
    "        current_prompts=list(map(lambda e:e[1],prompts[i:i+batch_size]))\n",
    "        current_qstn_ids=list(map(lambda e:e[0],prompts[i:i+batch_size]))\n",
    "        responses = answer_model(current_prompts,max_new_tokens=max_new_tokens, return_full_text=return_full_text)\n",
    "        current_ans_ids =list(map(lambda r:r[0]['generated_text'].split(':')[0][-1:].strip(),responses))\n",
    "        solutions += list(zip(current_qstn_ids,current_ans_ids))\n",
    "        \n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solution(filename,solution, task=''):\n",
    "    df = pd.DataFrame(solution,columns=['Question_ID','Answer_ID'])\n",
    "    df['Task'] = task\n",
    "    df.to_csv(filename,index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_soln = answer_questions('../data/Question_Submission.txt',answer_generator,compression_retriever,sample_size=-1,batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_solution('testing_result.csv',train_soln,'Phi-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pred = pd.read_csv('training_result.csv')\n",
    "# act = pd.read_csv('../data/Q_A_ID_training.csv')\n",
    "# act = act[act['Question_ID'].isin(pred['Question_ID'])]\n",
    "# len(act)\n",
    "# import numpy as np\n",
    "# pred['Answer_ID'][pred['Answer_ID'].isna()]\n",
    "# pred=pred.sort_values(by=\"Question_ID\").reset_index(drop=True)\n",
    "# act=act.sort_values(by=\"Question_ID\").reset_index(drop=True)\n",
    "# pred['Answer_ID']=pred['Answer_ID'].astype(int)\n",
    "# (pred['Answer_ID'] == act['Answer_ID']).mean()\n",
    "# |algo|score|\n",
    "# |--|--|\n",
    "# |similarity|0.6235455167693361|\n",
    "# |mmr|xxx|\n",
    "# VECTOR_STORE_NAME\n",
    "# pred[pred['Answer_ID'] != act['Answer_ID']].head()\n",
    "# context from datastore\n",
    "# question = q\n",
    "# docs = compression_retriever.invoke(question['question'])\n",
    "# context =  (' '.join(list(map(lambda d:d.page_content,docs)))).replace('\\n', '. ')\n",
    "# prompt = get_inference_prompt(question, context)\n",
    "# refined_prompt = prompt['question']\n",
    "# answer = prompt['answer']\n",
    "# print(refined_prompt)\n",
    "# gen_result = answer_generator(refined_prompt,max_new_tokens=128,return_full_text=False,)\n",
    "# \"option 1) Avoid monitoring neighbo\"\n",
    "# print(answer)\n",
    "# print(gen_result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
