---
common: 
  llm_model: 
    name: 'microsoft/phi-2'
    context_length: 2048
  embedding_model: 
    name: 'all-MiniLM-L6-v2.gguf2.f16.gguf'
    kwargs: 
      allow_download: True
  reranker_model: 'BAAI/bge-reranker-base'
  vectorstore:
    path: 'data/vectorstore/chromadb_512_32_all-MiniLM-L6-v2'
    #host and port are only used for ingest
    host: 'localhost'  
    port: -8000  
    k : 200   #50-training, 200-testing
  compression_retriever_top_n: 9   #1 for fine tunning , 9 for inference
  trained_model_dir: 'bin/driver-pretrained/'

data-ingest:
  n_jobs: 24
  index_chunk: 64
  textsplitter:
    chunk_size: 512
    overlap: 32
  documents: 
    path: 'data/rel18/'
    extensions: 
      - '*.docx'

training:
  epochs: 1
  max_steps: 1024
  n_jobs: 6
  lora_rank: 8
  learning_rate: 0.0001
  output_dir: 'bin/'
  prompt_bin_filename: 'bin/pickle/training_finetuning_prompt.pkl'
  data_filename: 'data/TeleQnA_training.txt'
  dataset_dir: 'data/driver-finetuning/'
  
inference:
    task: Phi-2
    run_mode: 1 #0- training only , 1-test only, 2- both train and test
    n_jobs: 6
    training_input_filename: 'data/TeleQnA_training.txt'
    testing_input_filename:  'data/Question_Submission.txt'
    training_output_filename: 'data/training_result.csv'
    testing_output_filename:  'data/testing_result.csv'
    training_prompt_bin_filename: 'bin/pickle/inference_training_prompt.pkl'
    testing_prompt_bin_filename: 'bin/pickle/inference_testing_prompt.pkl'
    training_results:  'data/Q_A_ID_training.csv'